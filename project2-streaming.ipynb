{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "270ec464",
   "metadata": {},
   "source": [
    "## DS-2002 Project 2: Healthcare Data Lakehouse with PySpark\n",
    "\n",
    "### Overview\n",
    "This notebook implements a multi-source data integration pipeline using PySpark and the Bronze-Silver-Gold medallion architecture. The pipeline processes healthcare appointment data from multiple sources and creates a dimensional data mart optimized for analytical queries.\n",
    "\n",
    "### Data Sources\n",
    "1. **MongoDB Atlas** - Doctors and Clinics (NoSQL)\n",
    "2. **MySQL** - Patients and Date Dimension (Relational)\n",
    "3. **JSON Files** - Appointments in 3 batches (File System)\n",
    "\n",
    "### Architecture\n",
    "- **Bronze Layer:** Raw appointment data from JSON batches\n",
    "- **Silver Layer:** Enriched appointments joined with dimension tables\n",
    "- **Gold Layer:** Final fact table with surrogate keys\n",
    "\n",
    "### Technologies\n",
    "- PySpark for distributed data processing\n",
    "- MongoDB for NoSQL data storage\n",
    "- MySQL for dimensional data mart\n",
    "- Pandas for data manipulation\n",
    "- SQLAlchemy for database connections\n",
    "\n",
    "### Business Value\n",
    "The data mart enables analysis of:\n",
    "- Revenue and appointment trends by department\n",
    "- Doctor performance metrics\n",
    "- Monthly appointment patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fee48f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "39de134f",
   "metadata": {},
   "source": [
    "#### Setup: Create Directory Structure and Split Appointments into Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dee8cc9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "base_dir = os.path.join(os.getcwd(), 'project_data')\n",
    "data_dir = os.path.join(base_dir, 'healthcare')\n",
    "stream_dir = os.path.join(data_dir, 'streaming')\n",
    "appointments_stream_dir = os.path.join(stream_dir, 'appointments')\n",
    "\n",
    "os.makedirs(appointments_stream_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a05ceb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split appointments into 3 JSON batches\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, text\n",
    "from dotenv import load_dotenv\n",
    "from urllib.parse import quote_plus\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "mysql_args = {\n",
    "    \"uid\": \"root\",\n",
    "    \"pwd\": quote_plus(os.getenv(\"MYSQL_PWD\")),\n",
    "    \"hostname\": \"localhost\",\n",
    "    \"dbname\": \"healthcare_src\"\n",
    "}\n",
    "\n",
    "def get_sql_dataframe(sql_query, **args):\n",
    "    conn_str = f\"mysql+pymysql://{args['uid']}:{args['pwd']}@{args['hostname']}/{args['dbname']}\"\n",
    "    sqlEngine = create_engine(conn_str, pool_recycle=3600)\n",
    "    connection = sqlEngine.connect()\n",
    "    dframe = pd.read_sql(text(sql_query), connection)\n",
    "    connection.close()\n",
    "    return dframe\n",
    "\n",
    "sql_appts = \"SELECT * FROM healthcare_src.appointments_src;\"\n",
    "df_appointments = get_sql_dataframe(sql_appts, **mysql_args)\n",
    "\n",
    "batch_1 = df_appointments.iloc[0:2].copy()\n",
    "batch_2 = df_appointments.iloc[2:4].copy()\n",
    "batch_3 = df_appointments.iloc[4:6].copy()\n",
    "\n",
    "for batch in [batch_1, batch_2, batch_3]:\n",
    "    batch['appointment_ts'] = batch['appointment_ts'].astype(str)\n",
    "\n",
    "batch_1.to_json(os.path.join(appointments_stream_dir, 'appointments_batch_1.json'), orient='records', lines=True)\n",
    "batch_2.to_json(os.path.join(appointments_stream_dir, 'appointments_batch_2.json'), orient='records', lines=True)\n",
    "batch_3.to_json(os.path.join(appointments_stream_dir, 'appointments_batch_3.json'), orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb6b42e",
   "metadata": {},
   "source": [
    "#### Import Libraries and Configure PySpark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06a3c04c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\spark-4.5.4-bin-hadoop3\\spark-3.5.4-bin-hadoop3\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "print(findspark.find())\n",
    "\n",
    "import certifi\n",
    "import pymongo\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window as W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4a9fe15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set configuration variables\n",
    "mongodb_args = {\n",
    "    \"user_name\": os.getenv(\"MONGODB_USERNAME\"),\n",
    "    \"password\": os.getenv(\"MONGODB_PWD\"),\n",
    "    \"cluster_name\": \"Cluster0\",\n",
    "    \"cluster_subnet\": \"egia6sc\",\n",
    "    \"cluster_location\": \"atlas\",\n",
    "    \"db_name\": \"healthcare\"\n",
    "}\n",
    "\n",
    "mysql_mart_conn_args = {\n",
    "    \"uid\": \"root\",\n",
    "    \"pwd\": quote_plus(os.getenv(\"MYSQL_PWD\")),\n",
    "    \"hostname\": \"localhost\",\n",
    "    \"dbname\": \"healthcare_mart\"\n",
    "}\n",
    "\n",
    "dest_database = \"healthcare_dlh\"\n",
    "sql_warehouse_dir = os.path.abspath('spark-warehouse')\n",
    "dest_database_dir = f\"{dest_database}.db\"\n",
    "database_dir = os.path.join(sql_warehouse_dir, dest_database_dir)\n",
    "\n",
    "appointments_output_bronze = os.path.join(database_dir, 'fact_appointments', 'bronze')\n",
    "appointments_output_silver = os.path.join(database_dir, 'fact_appointments', 'silver')\n",
    "appointments_output_gold = os.path.join(database_dir, 'fact_appointments', 'gold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28674525",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Spark Session\n",
    "worker_threads = f\"local[{int(os.cpu_count()/2)}]\"\n",
    "shuffle_partitions = int(os.cpu_count())\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName('Healthcare Streaming Data Lakehouse') \\\n",
    "    .master(worker_threads) \\\n",
    "    .config('spark.driver.memory', '4g') \\\n",
    "    .config('spark.executor.memory', '2g') \\\n",
    "    .config('spark.sql.shuffle.partitions', shuffle_partitions) \\\n",
    "    .config('spark.sql.warehouse.dir', sql_warehouse_dir) \\\n",
    "    .config(\"spark.jars.packages\", \"mysql:mysql-connector-java:8.0.33\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83976a7",
   "metadata": {},
   "source": [
    "#### Extract: Load Dimension Tables from Multiple Sources (MongoDB & MySQL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65de9296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mongoDB helper functions\n",
    "def get_mongo_client(**args):\n",
    "    if args[\"cluster_location\"] == \"atlas\":\n",
    "        connect_str = f\"mongodb+srv://{args['user_name']}:{args['password']}@{args['cluster_name']}.{args['cluster_subnet']}.mongodb.net\"\n",
    "        client = pymongo.MongoClient(connect_str, tlsCAFile=certifi.where())\n",
    "    return client\n",
    "\n",
    "def get_mongo_dataframe(mongo_client, db_name, collection, query):\n",
    "    db = mongo_client[db_name]\n",
    "    dframe = pd.DataFrame(list(db[collection].find(query)))\n",
    "    dframe.drop(['_id'], axis=1, inplace=True)\n",
    "    mongo_client.close()\n",
    "    return dframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "886006ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load doctors and clinics\n",
    "client = get_mongo_client(**mongodb_args)\n",
    "df_doctors_pandas = get_mongo_dataframe(client, mongodb_args[\"db_name\"], \"doctors\", {})\n",
    "df_dim_doctor = spark.createDataFrame(df_doctors_pandas) \\\n",
    "    .withColumnRenamed(\"id\", \"doctor_id\") \\\n",
    "    .select(\"doctor_id\", \"first_name\", \"last_name\", \"specialty\", \"department\", \"years_experience\", \"job_title\") \\\n",
    "    .withColumn(\"is_senior\", col(\"years_experience\") >= 10)\n",
    "\n",
    "client = get_mongo_client(**mongodb_args)\n",
    "df_clinics_pandas = get_mongo_dataframe(client, mongodb_args[\"db_name\"], \"clinics\", {})\n",
    "df_dim_clinic = spark.createDataFrame(df_clinics_pandas) \\\n",
    "    .select(\"clinic_id\", \"name\", \"department\", \"city\", \"state\") \\\n",
    "    .withColumn(\"location\", concat_ws(\", \", col(\"city\"), col(\"state\"))) \\\n",
    "    .select(\"clinic_id\", \"name\", \"department\", \"location\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "090036b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load patients and date dimension\n",
    "sql_patients = \"SELECT * FROM healthcare_mart.dimpatient;\"\n",
    "df_patients_pandas = get_sql_dataframe(sql_patients, **mysql_mart_conn_args)\n",
    "df_dim_patient = spark.createDataFrame(df_patients_pandas)\n",
    "\n",
    "sql_date = \"SELECT DateKey, DateValue FROM healthcare_mart.dim_date;\"\n",
    "df_date_pandas = get_sql_dataframe(sql_date, **mysql_mart_conn_args)\n",
    "df_dim_date = spark.createDataFrame(df_date_pandas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f200401",
   "metadata": {},
   "source": [
    "#### Bronze Layer: (Raw Data Ingestion) Ingest appointment JSON batches as 3 micro-batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dab40b98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bronze accumulated rows: 6\n"
     ]
    }
   ],
   "source": [
    "appointments_schema = StructType([\n",
    "    StructField(\"appointment_id\", IntegerType(), True),\n",
    "    StructField(\"doctor_id\", IntegerType(), True),\n",
    "    StructField(\"patient_id\", IntegerType(), True),\n",
    "    StructField(\"room_id\", IntegerType(), True),\n",
    "    StructField(\"insurance_id\", IntegerType(), True),\n",
    "    StructField(\"appointment_ts\", StringType(), True),\n",
    "    StructField(\"visit_type\", StringType(), True),\n",
    "    StructField(\"duration_minutes\", IntegerType(), True),\n",
    "    StructField(\"cost_usd\", DecimalType(10, 2), True)\n",
    "])\n",
    "\n",
    "\n",
    "checkpoint_dir = os.path.join(database_dir, \"_checkpoints\", \"appointments_bronze\")\n",
    "bronze_stream_out = appointments_output_bronze \n",
    "\n",
    "df_appointments_stream = (\n",
    "    spark.readStream\n",
    "        .schema(appointments_schema)\n",
    "        .option(\"maxFilesPerTrigger\", 1)  # 1 file per interval\n",
    "        .json(appointments_stream_dir)\n",
    ")\n",
    "\n",
    "# write Bronze as parquet\n",
    "bronze_query = (\n",
    "    df_appointments_stream.writeStream\n",
    "        .format(\"parquet\")\n",
    "        .option(\"path\", bronze_stream_out)\n",
    "        .option(\"checkpointLocation\", checkpoint_dir)\n",
    "        .trigger(availableNow=True) \n",
    "        .outputMode(\"append\")\n",
    "        .start()\n",
    ")\n",
    "\n",
    "bronze_query.awaitTermination()\n",
    "\n",
    "df_appointments_all = spark.read.parquet(bronze_stream_out)\n",
    "\n",
    "print(f\"Bronze accumulated rows: {df_appointments_all.count()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7feb464c",
   "metadata": {},
   "source": [
    "#### Silver Layer (Cleaned & Validated Data): Transform and Join with Dimension Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "948ee77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# join appointments with dimensions\n",
    "df_appointments_silver = df_appointments_all \\\n",
    "    .withColumn(\"appointment_ts\", to_timestamp(col(\"appointment_ts\"))) \\\n",
    "    .withColumn(\"DateKey\", date_format(col(\"appointment_ts\"), \"yyyyMMdd\").cast(\"int\")) \\\n",
    "    .join(df_dim_doctor.select(\"doctor_id\", \"department\"), \"doctor_id\", \"left\") \\\n",
    "    .join(df_dim_clinic.select(\"clinic_id\", \"department\"), \"department\", \"left\") \\\n",
    "    .select(\"appointment_id\", \"DateKey\", \"doctor_id\", \"patient_id\", \"clinic_id\", \n",
    "            \"duration_minutes\", \"cost_usd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95d7af7",
   "metadata": {},
   "source": [
    "#### Gold Layer (Analytics-Ready Data): Create Final Fact Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22a0ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Gold layer with window function\n",
    "windowSpec = W.orderBy(\"appointment_id\")\n",
    "df_appointments_gold = df_appointments_silver \\\n",
    "    .withColumn(\"AppointmentKey\", row_number().over(windowSpec)) \\\n",
    "    .select(\"AppointmentKey\", \"DateKey\", \"doctor_id\", \"patient_id\", \"clinic_id\", \"duration_minutes\", \"cost_usd\")\n",
    "\n",
    "# create temporary view (works with batch DataFrames)\n",
    "df_appointments_gold.createOrReplaceTempView(\"gold_appointments_temp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "68b201ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gold Table: 6 records\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AppointmentKey</th>\n",
       "      <th>appointment_id</th>\n",
       "      <th>DateKey</th>\n",
       "      <th>doctor_id</th>\n",
       "      <th>patient_id</th>\n",
       "      <th>clinic_id</th>\n",
       "      <th>duration_minutes</th>\n",
       "      <th>cost_usd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>20250103</td>\n",
       "      <td>101</td>\n",
       "      <td>201</td>\n",
       "      <td>1.0</td>\n",
       "      <td>30</td>\n",
       "      <td>120.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>20250103</td>\n",
       "      <td>101</td>\n",
       "      <td>202</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20</td>\n",
       "      <td>85.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>20250214</td>\n",
       "      <td>102</td>\n",
       "      <td>203</td>\n",
       "      <td>2.0</td>\n",
       "      <td>45</td>\n",
       "      <td>200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>20250301</td>\n",
       "      <td>103</td>\n",
       "      <td>204</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30</td>\n",
       "      <td>115.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>20250315</td>\n",
       "      <td>102</td>\n",
       "      <td>205</td>\n",
       "      <td>2.0</td>\n",
       "      <td>25</td>\n",
       "      <td>95.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>20250422</td>\n",
       "      <td>101</td>\n",
       "      <td>206</td>\n",
       "      <td>1.0</td>\n",
       "      <td>60</td>\n",
       "      <td>260.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   AppointmentKey  appointment_id   DateKey  doctor_id  patient_id  clinic_id  \\\n",
       "0               1               1  20250103        101         201        1.0   \n",
       "1               2               2  20250103        101         202        1.0   \n",
       "2               3               3  20250214        102         203        2.0   \n",
       "3               4               4  20250301        103         204        NaN   \n",
       "4               5               5  20250315        102         205        2.0   \n",
       "5               6               6  20250422        101         206        1.0   \n",
       "\n",
       "   duration_minutes  cost_usd  \n",
       "0                30     120.0  \n",
       "1                20      85.0  \n",
       "2                45     200.0  \n",
       "3                30     115.0  \n",
       "4                25      95.0  \n",
       "5                60     260.0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the base appointment data as pandas\n",
    "import json\n",
    "all_appointments = []\n",
    "for i in range(1, 4):\n",
    "    batch_file = os.path.join(appointments_stream_dir, f'appointments_batch_{i}.json')\n",
    "    with open(batch_file, 'r') as f:\n",
    "        for line in f:\n",
    "            all_appointments.append(json.loads(line))\n",
    "\n",
    "df_appts = pd.DataFrame(all_appointments)\n",
    "\n",
    "# convert timestamp and create DateKey\n",
    "df_appts['appointment_ts'] = pd.to_datetime(df_appts['appointment_ts'])\n",
    "df_appts['DateKey'] = df_appts['appointment_ts'].dt.strftime('%Y%m%d').astype(int)\n",
    "\n",
    "# join with doctors\n",
    "df_appts = df_appts.merge(\n",
    "    df_doctors_pandas[['id', 'department']].rename(columns={'id': 'doctor_id'}),\n",
    "    on='doctor_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# join with clinics \n",
    "df_appts = df_appts.merge(\n",
    "    df_clinics_pandas[['clinic_id', 'department']],\n",
    "    on='department',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# create Gold fact table\n",
    "df_gold_pandas = df_appts[['appointment_id', 'DateKey', 'doctor_id', 'patient_id', \n",
    "                            'clinic_id', 'duration_minutes', 'cost_usd']].copy()\n",
    "df_gold_pandas.insert(0, 'AppointmentKey', range(1, len(df_gold_pandas) + 1))\n",
    "\n",
    "print(f\"Gold Table: {len(df_gold_pandas)} records\")\n",
    "\n",
    "# write to MySQL\n",
    "conn_str = f\"mysql+pymysql://{mysql_mart_conn_args['uid']}:{mysql_mart_conn_args['pwd']}@{mysql_mart_conn_args['hostname']}/{mysql_mart_conn_args['dbname']}\"\n",
    "sqlEngine = create_engine(conn_str, pool_recycle=3600)\n",
    "connection = sqlEngine.connect()\n",
    "\n",
    "df_gold_pandas.to_sql(\"factappointment_v2\", con=connection, index=False, if_exists='replace')\n",
    "\n",
    "from sqlalchemy import text\n",
    "connection.execute(text(\"ALTER TABLE factappointment_v2 ADD PRIMARY KEY (AppointmentKey);\"))\n",
    "connection.close()\n",
    "\n",
    "df_gold_pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c85d1f",
   "metadata": {},
   "source": [
    "### Analytical Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3dc18d6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REVENUE AND APPOINTMENTS BY DEPARTMENT:\n",
      "   department  NumAppointments  TotalRevenue  AvgCost  AvgDuration\n",
      "0  Cardiology                3         465.0    155.0      36.6667\n",
      "1   Neurology                2         295.0    147.5      35.0000\n"
     ]
    }
   ],
   "source": [
    "# Query 1: Total revenue and appointments by department\n",
    "query1 = \"\"\"\n",
    "SELECT \n",
    "    c.department,\n",
    "    COUNT(f.AppointmentKey) AS NumAppointments,\n",
    "    SUM(f.cost_usd) AS TotalRevenue,\n",
    "    AVG(f.cost_usd) AS AvgCost,\n",
    "    AVG(f.duration_minutes) AS AvgDuration\n",
    "FROM healthcare_mart.factappointment_v2 f\n",
    "JOIN healthcare_mart.dimclinic c ON f.clinic_id = c.clinic_id\n",
    "GROUP BY c.department\n",
    "ORDER BY TotalRevenue DESC;\n",
    "\"\"\"\n",
    "\n",
    "df_q1 = get_sql_dataframe(query1, **mysql_mart_conn_args)\n",
    "print(\"REVENUE AND APPOINTMENTS BY DEPARTMENT:\")\n",
    "print(df_q1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "65e434ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOCTOR PERFORMANCE METRICS:\n",
      "  first_name last_name    specialty  is_senior  NumAppointments  TotalRevenue  \\\n",
      "0      Alice     Patel   Cardiology          1                3         465.0   \n",
      "1   Benjamin       Lee    Neurology          0                2         295.0   \n",
      "2      Carla     Gomez  Orthopedics          1                1         115.0   \n",
      "\n",
      "   AvgDuration  \n",
      "0      36.6667  \n",
      "1      35.0000  \n",
      "2      30.0000  \n"
     ]
    }
   ],
   "source": [
    "# Query 2: Doctor performance metrics\n",
    "query2 = \"\"\"\n",
    "SELECT \n",
    "    d.first_name,\n",
    "    d.last_name,\n",
    "    d.specialty,\n",
    "    d.is_senior,\n",
    "    COUNT(f.AppointmentKey) AS NumAppointments,\n",
    "    SUM(f.cost_usd) AS TotalRevenue,\n",
    "    AVG(f.duration_minutes) AS AvgDuration\n",
    "FROM healthcare_mart.factappointment_v2 f\n",
    "JOIN healthcare_mart.dimdoctor d ON f.doctor_id = d.doctor_id\n",
    "GROUP BY d.first_name, d.last_name, d.specialty, d.is_senior\n",
    "ORDER BY TotalRevenue DESC;\n",
    "\"\"\"\n",
    "\n",
    "df_q2 = get_sql_dataframe(query2, **mysql_mart_conn_args)\n",
    "print(\"DOCTOR PERFORMANCE METRICS:\")\n",
    "print(df_q2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7f4d5b58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MONTHLY APPOINTMENT TRENDS:\n",
      "  MonthName  YearNum  NumAppointments  TotalRevenue  AvgCost\n",
      "0   January     2025                2         205.0    102.5\n",
      "1  February     2025                1         200.0    200.0\n",
      "2     March     2025                2         210.0    105.0\n",
      "3     April     2025                1         260.0    260.0\n"
     ]
    }
   ],
   "source": [
    "# Query 3: Monthly appointment trends\n",
    "query3 = \"\"\"\n",
    "SELECT \n",
    "    dt.MonthName,\n",
    "    dt.YearNum,\n",
    "    COUNT(f.AppointmentKey) AS NumAppointments,\n",
    "    SUM(f.cost_usd) AS TotalRevenue,\n",
    "    AVG(f.cost_usd) AS AvgCost\n",
    "FROM healthcare_mart.factappointment_v2 f\n",
    "JOIN healthcare_mart.dim_date dt ON f.DateKey = dt.DateKey\n",
    "GROUP BY dt.MonthName, dt.YearNum, dt.MonthNum\n",
    "ORDER BY dt.YearNum, dt.MonthNum;\n",
    "\"\"\"\n",
    "\n",
    "df_q3 = get_sql_dataframe(query3, **mysql_mart_conn_args)\n",
    "print(\"MONTHLY APPOINTMENT TRENDS:\")\n",
    "print(df_q3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pysparkenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
